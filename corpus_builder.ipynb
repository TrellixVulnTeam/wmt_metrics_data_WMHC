{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_downloader\n",
    "import tempfile\n",
    "import os\n",
    "import sys\n",
    "from logging import getLogger\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import argparse\n",
    "logger = data_downloader.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WMT_IMPORTERS = {\n",
    "    \"15\": data_downloader.Importer1516,\n",
    "    \"16\": data_downloader.Importer1516,\n",
    "    \"17\": data_downloader.Importer17,\n",
    "    \"18\": data_downloader.Importer18,\n",
    "    \"19\": data_downloader.Importer19,\n",
    "    \"20\": data_downloader.Importer20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_flag(s):\n",
    "    \"\"\"\n",
    "    Parse boolean arguments from the command line.\n",
    "    \"\"\"\n",
    "    FALSY_STRINGS = {'off', 'false', '0'}\n",
    "    TRUTHY_STRINGS = {'on', 'true', '1'}\n",
    "    if s.lower() in FALSY_STRINGS:\n",
    "        return False\n",
    "    elif s.lower() in TRUTHY_STRINGS:\n",
    "        return True\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"Invalid value for a boolean flag!\")\n",
    "        \n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# path settings\n",
    "parser.add_argument('--target_path', type=str, default='/home/is/kosuke-t/scripts/make_data/wmt_metrics_data/data/wmt15-20_DA.json')\n",
    "parser.add_argument('--cache_path', type=str, default='/home/is/kosuke-t/scripts/make_data/wmt_metrics_data')\n",
    "parser.add_argument('--downloaded_dir', type=str, default='/home/is/kosuke-t/scripts/make_data/wmt_metrics_data/cache',\n",
    "                    help='for WMT20 submissions data. Must be specified when targeting WMT20')\n",
    "\n",
    "# others\n",
    "parser.add_argument('--years', type=str, default='15,16,17,18,19,20', help='separation must be given by \\\",\\\"')\n",
    "parser.add_argument('--target_language', type=str, default='*', help='if only english, then \\\"en\\\"')\n",
    "parser.add_argument('--include_unreliables', type=bool_flag, default=False,\n",
    "                    help='WMT20 has some unreliable data. This flag is set when including such data')\n",
    "parser.add_argument('--onlyMQM', type=bool_flag, default=False, \n",
    "                    help='only download and preprocessing MQM data. When both of onlyMQM and onlyPSQM are False, download DA data on WMT20')\n",
    "parser.add_argument('--onlyPSQM', type=bool_flag, default=False, \n",
    "                    help='only download and preprocessing PSQM data. When both of onlyMQM and onlyPSQM are False, download DA data on WMT20')\n",
    "parser.add_argument('--addMQM', type=bool_flag, default=False, help='build DA and MQM mixed data')\n",
    "parser.add_argument('--addPSQM', type=bool_flag, default=False, help='build DA and PSQM mixed data')\n",
    "parser.add_argument('--average_duplicates', type=bool_flag, default=True, \n",
    "                    help='Whether to take average of the scores annotated to the same sentences')\n",
    "parser.add_argument('--prevent_leaks', type=bool_flag, default=True, help='whether to allow for leaks among train and dev')\n",
    "parser.add_argument('--dev_ratio', type=float, default=0.1, help='development ratio')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.years = args.years.split(',')\n",
    "\n",
    "if args.addMQM:\n",
    "    assert (not args.onlyMQM) and (not args.onlyPSQM) and (not args.addPSQM) ,\\\n",
    "    'addMQM can stand only when other signals are off'\n",
    "if args.addPSQM:\n",
    "    assert (not args.onlyMQM) and (not args.onlyPSQM) and (not args.addMQM) ,\\\n",
    "    'addPSQM can stand only when other signals are off' \n",
    "\n",
    "if '20' in args.years:\n",
    "    assert os.path.isdir(args.downloaded_dir), 'Fetching 20\\'s data cannot be completed with this script.\\n'\\\n",
    "    'Download submission data from {} beforhand.\\n'\\\n",
    "    'Then, put the data folder inside the downloaded_dir of the arguments.'.format(data_downloader.WMT_LOCATIONS['20']['submissions'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wmt_dataset(target_file, rating_years, target_language):\n",
    "    \"\"\"Creates a JSONL file for a given set of years and a target language.\"\"\"\n",
    "    logger.info(\"*** Downloading ratings data from WMT.\")\n",
    "    assert target_file\n",
    "    assert not os.path.exists(args.target_path), \"Target file already exists. Aborting.\"\n",
    "    assert rating_years, \"No target year detected.\"\n",
    "    for year in rating_years:\n",
    "        assert year in WMT_IMPORTERS, \"No importer for year {}.\".format(year)\n",
    "    assert target_language\n",
    "    assert target_language == \"*\" or len(target_language) == 2, \"target_language must be a two-letter language code or `*`.\"\n",
    "    \n",
    "    with tempfile.TemporaryDirectory(dir=args.cache_path) as tmpdir:\n",
    "        logger.info(\"Using tmp directory: {}\".format(tmpdir))\n",
    "        args.cache_path = tmpdir\n",
    "        n_records_total = 0\n",
    "        tmp_file = os.path.join(tmpdir, \"tmp_ratings.json\")\n",
    "        \n",
    "        if '20' in rating_years:\n",
    "            logger.info('copying 20\\'s data to tmp directory...')\n",
    "            before_copy = os.path.join(args.downloaded_dir, data_downloader.WMT_LOCATIONS['20']['submissions'][0])\n",
    "            after_copy = os.path.join(tmpdir, data_downloader.WMT_LOCATIONS['20']['submissions'][0])\n",
    "            shutil.copytree(before_copy, after_copy)\n",
    "            logger.info('Done.')\n",
    "        \n",
    "        def fetch_and_generate(n_records_total):\n",
    "            # Builds an importer.\n",
    "            importer_class = WMT_IMPORTERS[year]\n",
    "            if year != '20':\n",
    "                importer = importer_class(year, tmp_file, tmpdir, args)\n",
    "            else:\n",
    "                importer = importer_class(year, tmp_file, tmpdir, args, args.include_unreliables, args.onlyMQM, args.onlyPSQM)\n",
    "            importer.fetch_files()\n",
    "            lang_pairs = importer.list_lang_pairs()\n",
    "            logger.info(\"Lang pairs found:\")\n",
    "            logger.info(\" \".join(lang_pairs))\n",
    "\n",
    "            for lang_pair in lang_pairs:\n",
    "\n",
    "                if target_language != \"*\" and not lang_pair.endswith(target_language):\n",
    "                    logger.info(\"Skipping language pair {}\".format(lang_pair))\n",
    "                    continue\n",
    "\n",
    "                logger.info(\"Generating records for {} and language pair {}\".format(year, lang_pair))\n",
    "                n_records = importer.generate_records_for_lang(lang_pair)\n",
    "                n_records_total += n_records\n",
    "            return n_records_total\n",
    "        \n",
    "        for year in rating_years:\n",
    "            logger.info(\"\\nProcessing ratings for year {}\".format(year))\n",
    "            \n",
    "            if year == '20' and args.addMQM:\n",
    "                n_records_total = fetch_and_generate(n_records_total)\n",
    "                args.onlyMQM = True\n",
    "                n_records_total = fetch_and_generate(n_records_total)\n",
    "            elif year == '20' and args.addPSQM:\n",
    "                n_records_total = fetch_and_generate(n_records_total)\n",
    "                args.onlyPSQM = True\n",
    "                n_records_total = fetch_and_generate(n_records_total)\n",
    "            else:\n",
    "                n_records_total = fetch_and_generate(n_records_total)\n",
    "\n",
    "        logger.info(\"Done processing {} elements\".format(n_records_total))\n",
    "        logger.info(\"Copying temp file...\")\n",
    "        shutil.copyfile(tmp_file, target_file)\n",
    "        logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(target_file, remove_null_refs=True, average_duplicates=True):\n",
    "    \"\"\"Postprocesses a JSONL file of ratings downloaded from WMT.\"\"\"\n",
    "    logger.info(\"\\n*** Post-processing WMT ratings {}.\".format(target_file))\n",
    "    base_file = target_file + \"_raw\"\n",
    "    if not os.path.isfile(base_file):\n",
    "        assert os.path.isfile(target_file), \"WMT ratings file not found!\"\n",
    "        os.replace(target_file, base_file)\n",
    "\n",
    "    logger.info(\"Reading and processing wmt data...\")\n",
    "    with open(base_file, \"r\") as f:\n",
    "        ratings_df = pd.read_json(f, lines=True)\n",
    "    # ratings_df = ratings_df[[\"lang\", \"reference\", \"candidate\", \"rating\"]]\n",
    "    ratings_df.rename(columns={\"rating\": \"score\"}, inplace=True)\n",
    "\n",
    "    if remove_null_refs:\n",
    "        ratings_df = ratings_df[ratings_df[\"reference\"].notnull()]\n",
    "        assert not ratings_df.empty\n",
    "\n",
    "    if average_duplicates:\n",
    "        try:\n",
    "            ratings_df = ratings_df.groupby(by=[\"lang\", \"source\", \"candidate\", \"reference\"]).agg({\"score\": \"mean\",}).reset_index()\n",
    "        except:\n",
    "            logger.info('No duplicates.')\n",
    "\n",
    "    logger.info(\"Saving clean file.\")\n",
    "    with open(target_file, \"w+\") as f:\n",
    "        ratings_df.to_json(f, orient=\"records\", lines=True)\n",
    "    logger.info(\"Cleaning up old ratings file.\")\n",
    "    os.remove(base_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle_no_leak(all_ratings_df, n_train):\n",
    "    \"\"\"Splits and shuffles such that there is no train/dev example with the same ref.\"\"\"\n",
    "\n",
    "    def is_split_leaky(ix):\n",
    "        return (all_ratings_df.iloc[ix].reference == all_ratings_df.iloc[ix-1].reference)\n",
    "\n",
    "    assert 0 < n_train < len(all_ratings_df.index)\n",
    "\n",
    "    # Clusters the examples by reference sentence.\n",
    "    sentences = all_ratings_df.reference.sample(frac=1, random_state=555).unique()\n",
    "    sentence_to_ix = {s: i for i, s in enumerate(sentences)}\n",
    "    all_ratings_df[\"__sentence_ix__\"] = [sentence_to_ix[s] for s in all_ratings_df.reference]\n",
    "    all_ratings_df = all_ratings_df.sort_values(by=\"__sentence_ix__\")\n",
    "    all_ratings_df.drop(columns=[\"__sentence_ix__\"], inplace=True)\n",
    "\n",
    "    # Moves the split point until there is no leakage.\n",
    "    split_ix = n_train\n",
    "    n_dev_sentences = len(all_ratings_df.iloc[split_ix:].reference.unique())\n",
    "    if n_dev_sentences == 1 and is_split_leaky(split_ix):\n",
    "        raise ValueError(\"Failed splitting data--not enough distinct dev sentences to prevent leak.\")\n",
    "    while is_split_leaky(split_ix):\n",
    "        split_ix += 1\n",
    "    if n_train != split_ix:\n",
    "        logger.info(\"Moved split point from {} to {} to prevent sentence leaking\".format(n_train, split_ix))\n",
    "\n",
    "    # Shuffles the train and dev sets separately.\n",
    "    train_ratings_df = all_ratings_df.iloc[:split_ix].copy()\n",
    "    train_ratings_df = train_ratings_df.sample(frac=1, random_state=555)\n",
    "    dev_ratings_df = all_ratings_df.iloc[split_ix:].copy()\n",
    "    dev_ratings_df = dev_ratings_df.sample(frac=1, random_state=555)\n",
    "    assert len(train_ratings_df) + len(dev_ratings_df) == len(all_ratings_df)\n",
    "\n",
    "    # Checks that there is no leakage.\n",
    "    train_sentences = train_ratings_df.reference.unique()\n",
    "    dev_sentences = dev_ratings_df.reference.unique()\n",
    "    logger.info(\"Using {} and {} unique sentences for train and dev.\".format(len(train_sentences), len(dev_sentences)))\n",
    "    assert not bool(set(train_sentences) & set(dev_sentences))\n",
    "\n",
    "    return train_ratings_df, dev_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle_leaky(all_ratings_df, n_train):\n",
    "    \"\"\"Shuffles and splits the ratings allowing overlap in the ref sentences.\"\"\"\n",
    "    all_ratings_df = all_ratings_df.sample(frac=1, random_state=555)\n",
    "    all_ratings_df = all_ratings_df.reset_index(drop=True)\n",
    "    train_ratings_df = all_ratings_df.iloc[:n_train].copy()\n",
    "    dev_ratings_df = all_ratings_df.iloc[n_train:].copy()\n",
    "    assert len(train_ratings_df) + len(dev_ratings_df) == len(all_ratings_df)\n",
    "    return train_ratings_df, dev_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_split(ratings_file,\n",
    "                  train_file=None,\n",
    "                  dev_file=None,\n",
    "                  dev_ratio=.1,\n",
    "                  prevent_leaks=True):\n",
    "    \"\"\"Splits a JSONL WMT ratings file into train/dev.\"\"\"\n",
    "    logger.info(\"\\n*** Splitting WMT data in train/dev.\")\n",
    "\n",
    "    assert os.path.isfile(ratings_file), \"WMT ratings file not found!\"\n",
    "    base_file = ratings_file + \"_raw\"\n",
    "    os.replace(ratings_file, base_file)\n",
    "\n",
    "    logger.info(\"Reading wmt data...\")\n",
    "    with open(base_file, \"r\") as f:\n",
    "        ratings_df = pd.read_json(f, lines=True)\n",
    "\n",
    "    logger.info(\"Doing the shuffle / split.\")\n",
    "    n_rows, n_train = len(ratings_df), int((1 - dev_ratio) * len(ratings_df))\n",
    "    logger.info(\"Will attempt to set aside {} out of {} rows for dev.\".format(n_rows - n_train, n_rows))\n",
    "    if prevent_leaks:\n",
    "        train_df, dev_df = _shuffle_no_leak(ratings_df, n_train)\n",
    "    else:\n",
    "        train_df, dev_df = _shuffle_leaky(ratings_df, n_train)\n",
    "    logger.info(\"Created train and dev files with {} and {} records.\".format(len(train_df), len(dev_df)))\n",
    "\n",
    "    logger.info(\"Saving clean file.\")\n",
    "    if not train_file:\n",
    "        train_file = ratings_file + \"_train\"\n",
    "    with open(train_file, \"w+\") as f:\n",
    "        train_df.to_json(f, orient=\"records\", lines=True)\n",
    "    if not dev_file:\n",
    "        dev_file = ratings_file + \"_dev\"\n",
    "    with open(dev_file, \"w+\") as f:\n",
    "        dev_df.to_json(f, orient=\"records\", lines=True)\n",
    "\n",
    "    logger.info(\"Cleaning up old ratings file.\")\n",
    "    os.remove(base_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*** Downloading ratings data from WMT.\n",
      "Using tmp directory: /home/is/kosuke-t/scripts/make_data/wmt_metrics_data/tmp_zq2khkw\n",
      "copying 20's data to tmp directory...\n",
      "Done.\n",
      "\n",
      "Processing ratings for year 15\n",
      "Checking cached tar file /home/is/kosuke-t/scripts/make_data/wmt_metrics_data/tmp_zq2khkw/DAseg-wmt-newstest2015.tar.gz.\n",
      "File not found in cache.\n",
      "Downloading DAseg-wmt-newstest2015 from http://www.computing.dcu.ie/~ygraham/DAseg-wmt-newstest2015.tar.gz\n",
      "Done.\n",
      "Untaring...\n",
      "Done.\n",
      "Lang pairs found:\n",
      "de-en fi-en ru-en cs-en en-ru\n",
      "Generating records for 15 and language pair de-en\n",
      "Processed 500 records of 2015's de-en\n",
      "Generating records for 15 and language pair fi-en\n",
      "Processed 500 records of 2015's fi-en\n",
      "Generating records for 15 and language pair ru-en\n",
      "Processed 500 records of 2015's ru-en\n",
      "Generating records for 15 and language pair cs-en\n",
      "Processed 500 records of 2015's cs-en\n",
      "Generating records for 15 and language pair en-ru\n",
      "Processed 500 records of 2015's en-ru\n",
      "\n",
      "Processing ratings for year 16\n",
      "Checking cached tar file /home/is/kosuke-t/scripts/make_data/wmt_metrics_data/tmp_zq2khkw/DAseg-wmt-newstest2016.tar.gz.\n",
      "File not found in cache.\n",
      "Downloading DAseg-wmt-newstest2016 from http://www.computing.dcu.ie/~ygraham/DAseg-wmt-newstest2016.tar.gz\n",
      "Done.\n",
      "Untaring...\n",
      "Done.\n",
      "Lang pairs found:\n",
      "de-en fi-en ru-en cs-en tr-en en-ru ro-en\n",
      "Generating records for 16 and language pair de-en\n",
      "Processed 560 records of 2016's de-en\n",
      "Generating records for 16 and language pair fi-en\n",
      "Processed 560 records of 2016's fi-en\n",
      "Generating records for 16 and language pair ru-en\n",
      "Processed 560 records of 2016's ru-en\n",
      "Generating records for 16 and language pair cs-en\n",
      "Processed 560 records of 2016's cs-en\n",
      "Generating records for 16 and language pair tr-en\n",
      "Processed 560 records of 2016's tr-en\n",
      "Generating records for 16 and language pair en-ru\n",
      "Processed 560 records of 2016's en-ru\n",
      "Generating records for 16 and language pair ro-en\n",
      "Processed 560 records of 2016's ro-en\n",
      "\n",
      "Processing ratings for year 17\n",
      "Checking cached tar file /home/is/kosuke-t/scripts/make_data/wmt_metrics_data/tmp_zq2khkw/wmt17-metrics-task-package.tgz.\n",
      "File not found in cache.\n",
      "Downloading wmt17-metrics-task-no-hybrids from http://ufallab.ms.mff.cuni.cz/~bojar/wmt17-metrics-task-package.tgz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a0f3226ac226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_wmt_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_duplicates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev_ratio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mshuffle_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevent_leaks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevent_leaks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ea68bde433ec>\u001b[0m in \u001b[0;36mcreate_wmt_dataset\u001b[0;34m(target_file, rating_years, target_language)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mn_records_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_and_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_records_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mn_records_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_and_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_records_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done processing {} elements\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_records_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ea68bde433ec>\u001b[0m in \u001b[0;36mfetch_and_generate\u001b[0;34m(n_records_total)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mimporter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimporter_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_unlilables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monlyMQM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monlyPSQM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mlang_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_lang_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Lang pairs found:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/make_data/wmt_metrics_data/data_downloader.py\u001b[0m in \u001b[0;36mfetch_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;34m\"\"\"Downloads the WMT eval files.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# Downloads the main archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImporter17\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m#Unpacks the segments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/make_data/wmt_metrics_data/data_downloader.py\u001b[0m in \u001b[0;36mfetch_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File not found in cache.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading {} from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_tar_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_tar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_tar_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/torch-tensor/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/torch-tensor/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/torch-tensor/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/torch-tensor/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_wmt_dataset(args.target_path, args.years, args.target_language)\n",
    "postprocess(args.target_path, average_duplicates=args.average_duplicates)\n",
    "if args.dev_ratio > 0.0:\n",
    "    shuffle_split(args.target_path, dev_ratio=args.dev_ratio, prevent_leaks=args.prevent_leaks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
